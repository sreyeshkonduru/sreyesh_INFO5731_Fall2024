{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreyeshkonduru/sreyesh_INFO5731_Fall2024/blob/main/Konduru_sreyesh_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5oNeL5pqsn0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "'''\n",
        "\n",
        "Research Question:\n",
        "How does the frequency of subreddit creation related to \"Data Science\" correlate with trends in the field,\n",
        "such as the number of publications, the rise of data-driven industries, and how the perspective changes from time to time about the data science?\n",
        "\n",
        "For data collection, I will be collecting subreddits from the reddit website.Using webscraping tools I will\n",
        "collect the data by creating my own API and using it to get access for scraping the data and then gather them so that we can make an analysis on that data.After gathering\n",
        "sufficient amount of data and save it into the CSV file.\n",
        "\n",
        "Data to be collected:\n",
        "Title: Name of the subreddit.\n",
        "Link: URL of the subreddit.\n",
        "Description: Overview of the subreddit.\n",
        "Created Date: The date when the subreddit was created.\n",
        "\n",
        "Amount of data needed for analysis:\n",
        "Atleast 1000 samples of data have to be collected from the reddit pages about the Data science.\n",
        "\n",
        "Steps to collect and save data:\n",
        "-Python script to extract subreddit information related to \"Data Science.\"\n",
        "-Extract the samples like title,link,description,Created data using webscraping methods\n",
        "-Sort the data for clear understaning and organized data\n",
        "-Append each parameters of data into one DataFrame\n",
        "-Save the data into a CSV file.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "7XEbe2Szm4Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "id": "pJBBVzOonpsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import praw\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# Create a Reddit instance\n",
        "reddit = praw.Reddit(\n",
        "    client_id='d08Kzfsjomooi05W27F1Pw',\n",
        "    client_secret='Zv5B76dzGq1IVAscnvnqgoJSIMWT_g',\n",
        "    user_agent='Sreyesh Varma/0.1 by Sreyesh Varma'\n",
        ")\n",
        "\n",
        "# Search for subreddits related to \"Data Science\"\n",
        "subreddits_data = []\n",
        "\n",
        "for subreddit in reddit.subreddits.search('Data science', limit=1000):\n",
        "    created_utc = subreddit.created_utc\n",
        "    created_date = datetime.datetime.utcfromtimestamp(created_utc).strftime('%Y-%m-%d')\n",
        "    subreddits_data.append({\n",
        "        'Title': subreddit.display_name,\n",
        "        'Link': f'https://www.reddit.com/r/{subreddit.display_name}',\n",
        "        'Description': subreddit.public_description,\n",
        "        'Created Date': created_date\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(subreddits_data)\n",
        "\n",
        "# Save to CSV file\n",
        "df.to_csv('data_science_subreddits.csv', index=False)\n",
        "\n",
        "print(df.head(15))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knK1prLQXAN_",
        "outputId": "160cdd8a-f08d-4ceb-ee58-d29d5d976eb8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Title                                            Link  \\\n",
            "0             datascience            https://www.reddit.com/r/datascience   \n",
            "1         DataScienceJobs        https://www.reddit.com/r/DataScienceJobs   \n",
            "2     DataScienceProjects    https://www.reddit.com/r/DataScienceProjects   \n",
            "3                 science                https://www.reddit.com/r/science   \n",
            "4         dataengineering        https://www.reddit.com/r/dataengineering   \n",
            "5        DataScienceMemes       https://www.reddit.com/r/DataScienceMemes   \n",
            "6     DataScienceStudents    https://www.reddit.com/r/DataScienceStudents   \n",
            "7         dataisbeautiful        https://www.reddit.com/r/dataisbeautiful   \n",
            "8            dataanalysis           https://www.reddit.com/r/dataanalysis   \n",
            "9        DataScienceGuide       https://www.reddit.com/r/DataScienceGuide   \n",
            "10               DataCamp               https://www.reddit.com/r/DataCamp   \n",
            "11   learnmachinelearning   https://www.reddit.com/r/learnmachinelearning   \n",
            "12             statistics             https://www.reddit.com/r/statistics   \n",
            "13  DataScienceSimplified  https://www.reddit.com/r/DataScienceSimplified   \n",
            "14      ScienceUncensored      https://www.reddit.com/r/ScienceUncensored   \n",
            "\n",
            "                                          Description Created Date  \n",
            "0   A space for data science professionals to enga...   2011-08-06  \n",
            "1   A place for people to post data science/machin...   2013-05-23  \n",
            "2   A subreddit for sharing progress on data scien...   2016-10-27  \n",
            "3   This community is a place to share and discuss...   2006-10-18  \n",
            "4   News & discussion on Data Engineering topics, ...   2015-02-06  \n",
            "5          Memes of Data Science and Machine Learning   2018-11-12  \n",
            "6   This is a subreddit for any Data Science stude...   2017-02-23  \n",
            "7   DataIsBeautiful is for visualizations that eff...   2012-02-14  \n",
            "8   This is a place to discuss and post about data...   2014-07-29  \n",
            "9   News and announcements for http://DataScienceG...   2016-01-20  \n",
            "10  Learn Data Science from the comfort of your br...   2016-07-08  \n",
            "11  A subreddit dedicated to learning machine lear...   2016-02-23  \n",
            "12  /r/Statistics is going dark from June 12-14th ...   2008-03-13  \n",
            "13   Beginner friendly community that is all about...   2017-06-04  \n",
            "14  ScienceUncensored is a place for anyone to pos...   2012-07-08  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_articles(search_term, num_results):\n",
        "    scholar_url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C44&q=XYZ&btnG=\"\n",
        "    search_params = {\n",
        "        'hl': 'en',\n",
        "        'q': search_term,\n",
        "        'as_ylo': '2014',\n",
        "        'as_yhi': '2024'\n",
        "    }\n",
        "\n",
        "    webpage = requests.get(scholar_url, params=search_params)\n",
        "    parsed_page = BeautifulSoup(webpage.content, 'html.parser')\n",
        "\n",
        "    research_papers = []\n",
        "\n",
        "    for paper in parsed_page.find_all('div', class_='gs_ri')[:num_results]:\n",
        "        paper_title_tag = paper.find('h3', class_='gs_rt')\n",
        "        paper_authors_tag = paper.find('div', class_='gs_a')\n",
        "        paper_abstract_tag = paper.find('div', class_='gs_rs')\n",
        "\n",
        "        paper_title = paper_title_tag.text if paper_title_tag else \"N/A\"\n",
        "        journal_name = paper_authors_tag.text.split('-')[1].strip() if paper_authors_tag and len(paper_authors_tag.text.split('-')) > 1 else \"N/A\"\n",
        "        publication_year = paper_authors_tag.text.split()[-1] if paper_authors_tag else \"N/A\"\n",
        "        author_names = ', '.join(paper_authors_tag.text.split('-')[0].split(', ')) if paper_authors_tag else \"N/A\"\n",
        "        paper_abstract = paper_abstract_tag.text if paper_abstract_tag else \"N/A\"\n",
        "\n",
        "        research_papers.append({\n",
        "            'Title': paper_title,\n",
        "            'Journal': journal_name,\n",
        "            'Year': publication_year,\n",
        "            'Authors': author_names,\n",
        "            'Abstract': paper_abstract\n",
        "        })\n",
        "\n",
        "    return research_papers\n",
        "\n",
        "search_query = \"natural language processing\"\n",
        "fetched_articles = fetch_articles(search_query, 1000)\n",
        "\n",
        "articles_df = pd.DataFrame(fetched_articles)\n",
        "articles_df.to_csv('research_articles.csv', index=False)\n",
        "\n",
        "print(articles_df.head(15))\n"
      ],
      "metadata": {
        "id": "-8VB8X6L-T9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb112717-62de-4f62-e121-9e5c973a46c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title  \\\n",
            "0                        Natural language processing   \n",
            "1  [HTML][HTML] Natural language processing: stat...   \n",
            "2            Advances in natural language processing   \n",
            "3  A primer on neural network models for natural ...   \n",
            "4  [PDF][PDF] The Stanford CoreNLP natural langua...   \n",
            "5  Transformers: State-of-the-art natural languag...   \n",
            "6  [BOOK][B] Neural network methods in natural la...   \n",
            "7  Allennlp: A deep semantic natural language pro...   \n",
            "8  Stanza: A Python natural language processing t...   \n",
            "9  Jumping NLP curves: A review of natural langua...   \n",
            "\n",
            "                                             Journal                 Year  \\\n",
            "0      Fundamentals of artificial intelligence, 2020             Springer   \n",
            "1            Multimedia tools and applications, 2023             Springer   \n",
            "2                                      Science, 2015          science.org   \n",
            "3  Journal of Artificial Intelligence Research, 2016             jair.org   \n",
            "4                        Proceedings of 52nd …, 2014     aclanthology.org   \n",
            "5                      … language processing …, 2020     aclanthology.org   \n",
            "6                                               2017     books.google.com   \n",
            "7                       arXiv preprint arXiv …, 2018            arxiv.org   \n",
            "8                       arXiv preprint arXiv …, 2020            arxiv.org   \n",
            "9            IEEE Computational intelligence …, 2014  ieeexplore.ieee.org   \n",
            "\n",
            "                                     Authors  \\\n",
            "0                KR Chowdhary, KR Chowdhary    \n",
            "1     D Khurana, A Koli, K Khatter, S Singh    \n",
            "2                  J Hirschberg, CD Manning    \n",
            "3                                Y Goldberg    \n",
            "4          CD Manning, M Surdeanu, J Bauer…    \n",
            "5      T Wolf, L Debut, V Sanh, J Chaumond…    \n",
            "6                                Y Goldberg    \n",
            "7  M Gardner, J Grus, M Neumann, O Tafjord…    \n",
            "8         P Qi, Y Zhang, Y Zhang, J Bolton…    \n",
            "9                        E Cambria, B White    \n",
            "\n",
            "                                            Abstract  \n",
            "0  … Python is also available with an extensive s...  \n",
            "1  … in machine specific language, Natural Langua...  \n",
            "2  … Natural language processing employs computat...  \n",
            "3  … More recently, neural network models started...  \n",
            "4  We describe the design and use of the Stanford...  \n",
            "5  … Recent progress in natural language processi...  \n",
            "6  … The series consists of 50- to 150-page monog...  \n",
            "7  This paper describes AllenNLP, a platform for ...  \n",
            "8  … open-source Python natural language processi...  \n",
            "9  … a deep understanding of natural language by ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "I have used ParseHub for this question and used reddit deals page to extract data\n",
        "\n",
        "steps for webscrapping through parseHub:\n",
        "\n",
        "1.First download the parseHub app from their website and complete the setup to install Parsehub in computer.\n",
        "\n",
        "2.Then, open ParseHub and click on \"New Project\".\n",
        "\n",
        "3.Paste the URL of the website you want to scrape. In this case, it is https://old.reddit.com/r/deals/\n",
        "\n",
        "4.Then, using select feature, click on the element that we want to extract.\n",
        "\n",
        "5.To extract more elements use relative select and select the elements.\n",
        "\n",
        "6.After selecting, extract data and then select run to get the data.\n",
        "\n",
        "7.Then select the type of file to download such as CSV file.\n",
        "\n",
        "Here is the File:\n",
        "https://myunt-my.sharepoint.com/:b:/g/personal/sreyeshvarmakonduru_my_unt_edu/EeAIcn7eFHpCgJauAO0bpBgBzHknEBaG-T2eO_wqGR-d8w?e=mOI7i3"
      ],
      "metadata": {
        "id": "BiDGvgEWFGvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I have listened to week 4 class about webscrapping. But the call activity that is given to us does not seem to be possible. I was getting so many errors and most of the websites\n",
        "are not giving access to scrape the data.After so many tries using different kinds of websites(even after trying old versions of websites),I finally got the outputs.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}