{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreyeshkonduru/sreyesh_INFO5731_Fall2024/blob/main/KONDURU_SREYESH_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "fec6c623-ec5f-402c-a249-b3817c99ff66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected papers for offset 0.\n",
            "Collected papers for offset 100.\n",
            "Collected papers for offset 200.\n",
            "Collected papers for offset 300.\n",
            "Collected papers for offset 400.\n",
            "Collected papers for offset 500.\n",
            "Collected papers for offset 600.\n",
            "Collected papers for offset 700.\n",
            "Collected papers for offset 800.\n",
            "Collected papers for offset 900.\n",
            "Collected abstracts. Saved to machine_learning_abstracts_semantic_scholar.csv.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Title  \\\n",
              "0   Fashion-MNIST: a Novel Image Dataset for Bench...   \n",
              "1   TensorFlow: A system for large-scale machine l...   \n",
              "2   TensorFlow: Large-Scale Machine Learning on He...   \n",
              "3   Stop explaining black box machine learning mod...   \n",
              "4   Convolutional LSTM Network: A Machine Learning...   \n",
              "5                 An Introduction to Machine Learning   \n",
              "6   A Survey on Bias and Fairness in Machine Learning   \n",
              "7   Open Graph Benchmark: Datasets for Machine Lea...   \n",
              "8   Machine learning: Trends, perspectives, and pr...   \n",
              "9                     Foundations of Machine Learning   \n",
              "10                  Physics-informed machine learning   \n",
              "11                     Interpretable Machine Learning   \n",
              "12  Membership Inference Attacks Against Machine L...   \n",
              "13  Data Mining Practical Machine Learning Tools a...   \n",
              "14     Machine learning - a probabilistic perspective   \n",
              "\n",
              "                                             Abstract  \n",
              "0   We present Fashion-MNIST, a new dataset compri...  \n",
              "1   TensorFlow is a machine learning system that o...  \n",
              "2   TensorFlow is an interface for expressing mach...  \n",
              "3                                                None  \n",
              "4   The goal of precipitation nowcasting is to pre...  \n",
              "5                                                None  \n",
              "6   With the widespread use of artificial intellig...  \n",
              "7   We present the Open Graph Benchmark (OGB), a d...  \n",
              "8   Machine learning addresses the question of how...  \n",
              "9                                                None  \n",
              "10                                               None  \n",
              "11  Interpretable machine learning has become a po...  \n",
              "12  We quantitatively investigate how machine lear...  \n",
              "13                                               None  \n",
              "14  All rights reserved. No part of this book may ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2eeb3d9-1a9c-43df-b9f2-e50b6642518f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fashion-MNIST: a Novel Image Dataset for Bench...</td>\n",
              "      <td>We present Fashion-MNIST, a new dataset compri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TensorFlow: A system for large-scale machine l...</td>\n",
              "      <td>TensorFlow is a machine learning system that o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TensorFlow: Large-Scale Machine Learning on He...</td>\n",
              "      <td>TensorFlow is an interface for expressing mach...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stop explaining black box machine learning mod...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Convolutional LSTM Network: A Machine Learning...</td>\n",
              "      <td>The goal of precipitation nowcasting is to pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>An Introduction to Machine Learning</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>A Survey on Bias and Fairness in Machine Learning</td>\n",
              "      <td>With the widespread use of artificial intellig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Open Graph Benchmark: Datasets for Machine Lea...</td>\n",
              "      <td>We present the Open Graph Benchmark (OGB), a d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Machine learning: Trends, perspectives, and pr...</td>\n",
              "      <td>Machine learning addresses the question of how...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Foundations of Machine Learning</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Physics-informed machine learning</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Interpretable Machine Learning</td>\n",
              "      <td>Interpretable machine learning has become a po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Membership Inference Attacks Against Machine L...</td>\n",
              "      <td>We quantitatively investigate how machine lear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Data Mining Practical Machine Learning Tools a...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Machine learning - a probabilistic perspective</td>\n",
              "      <td>All rights reserved. No part of this book may ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2eeb3d9-1a9c-43df-b9f2-e50b6642518f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d2eeb3d9-1a9c-43df-b9f2-e50b6642518f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d2eeb3d9-1a9c-43df-b9f2-e50b6642518f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-182c4394-90ed-4966-9f2c-1485b40a9afa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-182c4394-90ed-4966-9f2c-1485b40a9afa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-182c4394-90ed-4966-9f2c-1485b40a9afa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 979,\n        \"samples\": [\n          \"Machine learning in acoustics: Theory and applications.\",\n          \"Ensemble Methods in Machine Learning\",\n          \"A Survey of Deep Learning and Its Applications: A New Paradigm to Machine Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 718,\n        \"samples\": [\n          \"Intrusion detection is one of the important security problems in todays cyber world. A significant number of techniques have been developed which are based on machine learning approaches. However, they are not very successful in identifying all types of intrusions. In this paper, a detailed investigation and analysis of various machine learning techniques have been carried out for finding the cause of problems associated with various machine learning techniques in detecting intrusive activities. Attack classification and mapping of the attack features is provided corresponding to each attack. Issues which are related to detecting low-frequency attacks using network attack dataset are also discussed and viable methods are suggested for improvement. Machine learning techniques have been analyzed and compared in terms of their detection capability for detecting the various category of attacks. Limitations associated with each category of them are also discussed. Various data mining tools for machine learning have also been included in the paper. At the end, future directions are provided for attack detection using machine learning techniques.\",\n          \"Learning programs is a timely and interesting challenge. In Programming by Example (PBE), a system attempts to infer a program from input and output examples alone, by searching for a composition of some set of base functions. We show how machine learning can be used to speed up this seemingly hopeless search problem, by learning weights that relate textual features describing the provided input-output examples to plausible sub-components of a program. This generic learning framework lets us address problems beyond the scope of earlier PBE systems. Experiments on a prototype implementation show that learning improves search and ranking on a variety of text processing tasks found on help forums.\",\n          \"Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the\\\"Rashomon set\\\"of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "api_key = \"x2X70VOLvD9dgtxOzZ3Mc50QWwpNPsgu8Kw2pIwR\"\n",
        "\n",
        "headers = {\n",
        "    \"Accept\": \"application/json\",\n",
        "    \"x-api-key\": api_key\n",
        "}\n",
        "\n",
        "# Query to search\n",
        "query = \"machine learning\"\n",
        "\n",
        "api_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "# Parameters for the request\n",
        "params = {\n",
        "    \"query\": query,\n",
        "    \"fields\": \"title,abstract\",\n",
        "    \"limit\": 100  # Limit to 100 papers per request\n",
        "}\n",
        "\n",
        "# List to store all collected papers\n",
        "all_papers = []\n",
        "\n",
        "\n",
        "for i in range(0, 1000, 100):\n",
        "    params[\"offset\"] = i\n",
        "    response = requests.get(api_url, headers=headers, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        papers = response.json().get('data', [])\n",
        "        all_papers.extend(papers)\n",
        "        print(f\"Collected papers for offset {i}.\")\n",
        "    elif response.status_code == 429:\n",
        "        print(f\"Rate limit exceeded. Waiting 1 second before retrying for offset {i}\")\n",
        "        time.sleep(1)\n",
        "        response = requests.get(api_url, headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            papers = response.json().get('data', [])\n",
        "            all_papers.extend(papers)\n",
        "            print(f\"Collected papers after retry for offset {i}.\")\n",
        "        else:\n",
        "            print(f\"Error after retrying: {response.status_code}, Offset: {i}\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Error fetching data: {response.status_code}, Offset: {i}\")\n",
        "        break\n",
        "\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "# Extract titles and abstracts\n",
        "abstracts = [(paper[\"title\"], paper.get(\"abstract\", \"N/A\")) for paper in all_papers]\n",
        "\n",
        "# Save to CSV file\n",
        "csv_file = \"machine_learning_abstracts_semantic_scholar.csv\"\n",
        "df = pd.DataFrame(abstracts, columns=[\"Title\", \"Abstract\"])\n",
        "df.to_csv(csv_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Collected abstracts. Saved to {csv_file}.\")\n",
        "df.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "0_1ueouAXpV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c61ad2e-a8a1-43cf-f9d2-14ff6bb4870f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "wsvTaNW7YvDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7167b06-5ee1-4596-d165-e9c1327e84c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "id": "lWOSqr6oYRlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be1d19e-02f2-49ab-fb59-5a695587b8f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71aa028a-762f-4cb4-a04d-48dd426bc3a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to cleaned_research_abstracts.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "import os\n",
        "import platform\n",
        "\n",
        "# Download necessary NLTK resources if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the CSV file (adjusting for file paths based on OS)\n",
        "csv_file = \"machine_learning_abstracts_semantic_scholar.csv\"\n",
        "if platform.system() == \"Windows\":\n",
        "    csv_file = os.path.join(os.getcwd(), csv_file)\n",
        "\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Initialize necessary components\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to clean text data\n",
        "def clean_text(text):\n",
        "    # Check if the text is NaN or not a string, return an empty string if true\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # (1) Remove noise, such as special characters and punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # (2) Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # (3) Remove stopwords\n",
        "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
        "\n",
        "    # (4) Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    # (5) Stemming\n",
        "    text = ' '.join(stemmer.stem(word) for word in text.split())\n",
        "\n",
        "    # (6) Lemmatization using TextBlob's Word method\n",
        "    text = ' '.join(str(Word(word).lemmatize()) for word in text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to the 'Abstract' column\n",
        "df['Cleaned_Abstract'] = df['Abstract'].apply(clean_text)\n",
        "\n",
        "# Save the cleaned data to a new CSV file (handling file paths based on OS)\n",
        "cleaned_csv_file = \"cleaned_research_abstracts.csv\"\n",
        "if platform.system() == \"Windows\":\n",
        "    cleaned_csv_file = os.path.join(os.getcwd(), cleaned_csv_file)\n",
        "\n",
        "df.to_csv(cleaned_csv_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Cleaned data saved to {cleaned_csv_file}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "2rLU5JcSaAee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aebac5d-0a3d-4f6d-c41f-cbc49e5e4cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dependency Parsing for the first sentence in the first abstract:\n",
            "\n",
            "Sentence: present fashionmnist new dataset compris x grayscal imag fashion product categori imag per categori train set imag test set imag fashionmnist intend serv direct dropin replac origin mnist dataset benchmark machin learn algorithm share imag size data format structur train test split dataset freeli avail http url\n",
            "Token: present, Dependency: amod, Head: product\n",
            "Token: fashionmnist, Dependency: amod, Head: product\n",
            "Token: new, Dependency: amod, Head: dataset\n",
            "Token: dataset, Dependency: compound, Head: product\n",
            "Token: compris, Dependency: nmod, Head: product\n",
            "Token: x, Dependency: punct, Head: product\n",
            "Token: grayscal, Dependency: amod, Head: imag\n",
            "Token: imag, Dependency: amod, Head: product\n",
            "Token: fashion, Dependency: compound, Head: product\n",
            "Token: product, Dependency: nsubj, Head: set\n",
            "Token: categori, Dependency: aux, Head: set\n",
            "Token: imag, Dependency: acl, Head: categori\n",
            "Token: per, Dependency: prep, Head: imag\n",
            "Token: categori, Dependency: compound, Head: train\n",
            "Token: train, Dependency: nsubj, Head: set\n",
            "Token: set, Dependency: ROOT, Head: set\n",
            "Token: imag, Dependency: amod, Head: test\n",
            "Token: test, Dependency: dobj, Head: set\n",
            "Token: set, Dependency: dep, Head: set\n",
            "Token: imag, Dependency: amod, Head: fashionmnist\n",
            "Token: fashionmnist, Dependency: nsubj, Head: intend\n",
            "Token: intend, Dependency: conj, Head: set\n",
            "Token: serv, Dependency: nmod, Head: origin\n",
            "Token: direct, Dependency: amod, Head: origin\n",
            "Token: dropin, Dependency: nmod, Head: origin\n",
            "Token: replac, Dependency: compound, Head: origin\n",
            "Token: origin, Dependency: compound, Head: mnist\n",
            "Token: mnist, Dependency: compound, Head: machin\n",
            "Token: dataset, Dependency: compound, Head: benchmark\n",
            "Token: benchmark, Dependency: compound, Head: machin\n",
            "Token: machin, Dependency: dobj, Head: intend\n",
            "Token: learn, Dependency: npadvmod, Head: set\n",
            "Token: algorithm, Dependency: compound, Head: share\n",
            "Token: share, Dependency: ccomp, Head: learn\n",
            "Token: imag, Dependency: amod, Head: format\n",
            "Token: size, Dependency: compound, Head: format\n",
            "Token: data, Dependency: compound, Head: format\n",
            "Token: format, Dependency: compound, Head: test\n",
            "Token: structur, Dependency: compound, Head: test\n",
            "Token: train, Dependency: compound, Head: test\n",
            "Token: test, Dependency: nsubj, Head: split\n",
            "Token: split, Dependency: compound, Head: url\n",
            "Token: dataset, Dependency: compound, Head: url\n",
            "Token: freeli, Dependency: amod, Head: http\n",
            "Token: avail, Dependency: compound, Head: http\n",
            "Token: http, Dependency: compound, Head: url\n",
            "Token: url, Dependency: dobj, Head: set\n",
            "\n",
            "POS Counts for the first abstract:\n",
            "{'Nouns': 27, 'Verbs': 6, 'Adjectives': 12, 'Adverbs': 0}\n",
            "\n",
            "Named Entities for the first abstract:\n",
            "{'PERSON': 1, 'NORP': 1}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk import ne_chunk\n",
        "from nltk.tree import Tree\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "cleaned_csv_file = \"cleaned_research_abstracts.csv\"\n",
        "df = pd.read_csv(cleaned_csv_file.encode('utf-8').decode('utf-8'))\n",
        "\n",
        "# Initialize spaCy model for NER and parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform POS tagging and count POS types\n",
        "def pos_analysis(text):\n",
        "    # Ensure the text is a string\n",
        "    if not isinstance(text, str):\n",
        "        return [], {}\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    counts = {\n",
        "        'Nouns': sum(1 for word, tag in tagged if tag.startswith('NN')),\n",
        "        'Verbs': sum(1 for word, tag in tagged if tag.startswith('VB')),\n",
        "        'Adjectives': sum(1 for word, tag in tagged if tag.startswith('JJ')),\n",
        "        'Adverbs': sum(1 for word, tag in tagged if tag.startswith('RB')),\n",
        "    }\n",
        "\n",
        "    return tagged, counts\n",
        "\n",
        "# Function to perform constituency and dependency parsing\n",
        "def parsing_analysis(text):\n",
        "    if not isinstance(text, str):\n",
        "        return None\n",
        "    return nlp(text)\n",
        "\n",
        "# Function to extract named entities and their counts\n",
        "def ner_analysis(text):\n",
        "    if not isinstance(text, str):\n",
        "        return {}\n",
        "    doc = nlp(text)\n",
        "    entities = {}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        entities[ent.label_] = entities.get(ent.label_, 0) + 1\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Perform analysis for each abstract and store results\n",
        "results = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    abstract = row['Cleaned_Abstract']\n",
        "\n",
        "    # Skip empty abstracts\n",
        "    if pd.isna(abstract) or abstract.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    # POS Analysis\n",
        "    tagged, pos_counts = pos_analysis(abstract)\n",
        "\n",
        "    # Parsing Analysis\n",
        "    parsed_doc = parsing_analysis(abstract)\n",
        "\n",
        "    # NER Analysis\n",
        "    entities = ner_analysis(abstract)\n",
        "\n",
        "    results.append({\n",
        "        'Abstract': abstract,\n",
        "        'POS Tagged': tagged,\n",
        "        'POS Counts': pos_counts,\n",
        "        'Dependency Parsing': [(sent.text, [(token.text, token.dep_, token.head.text) for token in sent]) for sent in parsed_doc.sents] if parsed_doc else [],\n",
        "        'Named Entities': entities\n",
        "    })\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"cleaned_data.csv\", index=False)\n",
        "\n",
        "# Example Output of Parsing Analysis for the first abstract\n",
        "if results:\n",
        "    first_abstract = results[0]['Abstract']\n",
        "    parsed_doc = parsing_analysis(first_abstract)\n",
        "\n",
        "    # Print the dependency parse tree for the first sentence\n",
        "    print(f\"\\nDependency Parsing for the first sentence in the first abstract:\")\n",
        "    for sent in parsed_doc.sents:\n",
        "        print(f\"\\nSentence: {sent.text}\")\n",
        "        for token in sent:\n",
        "            print(f\"Token: {token.text}, Dependency: {token.dep_}, Head: {token.head.text}\")\n",
        "\n",
        "    # Print out POS counts for the first abstract\n",
        "    print(f\"\\nPOS Counts for the first abstract:\")\n",
        "    print(results[0]['POS Counts'])\n",
        "\n",
        "    # Print out Named Entities for the first abstract\n",
        "    print(f\"\\nNamed Entities for the first abstract:\")\n",
        "    print(results[0]['Named Entities'])\n",
        "else:\n",
        "    print(\"No valid abstracts found for analysis.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cleaned_data.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "38mQC7djxsdC",
        "outputId": "37328824-cdba-4627-b6f0-1696c18db474"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_908ed95b-2379-4376-979d-d1eaaaf39ac8\", \"cleaned_data.csv\", 4635385)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here is the cleaned data CSV:\n",
        "https://myunt-my.sharepoint.com/:x:/g/personal/sreyeshvarmakonduru_my_unt_edu/EdZp38U6THFKkbbIDfIIwxIBs-ZMQKX14WaWN4n-zq-ovw?e=43EkZq\n",
        "'''"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below"
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}