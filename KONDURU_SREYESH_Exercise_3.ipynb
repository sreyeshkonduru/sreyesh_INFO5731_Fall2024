{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreyeshkonduru/sreyesh_INFO5731_Fall2024/blob/main/KONDURU_SREYESH_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Title: CUSTOMER TICKET CLASSIFICATION TASK\n",
        "Using this classification task we can differentiate betwwen the different types of customer tickets like technical issues, paymment issues, product issues,accouunt problems etc.\n",
        "\n",
        "List of Features in this task:\n",
        "1. Text Length (Word Count):\n",
        "This feature measures the number of words in the combined subject and body of the ticket.\n",
        "\n",
        "2. Number of Exclamation Marks:\n",
        "It givves The count of exclamation marks in the text.\n",
        "\n",
        "3. Presence of Urgent Words:\n",
        "A  feature that checks for the presence of urgent words such as \"urgent,\" \"immediately,\" or \"asap.\"\n",
        "\n",
        "4. Sentiment Score:\n",
        "The overall sentiment polarity score of the entire text, calculated using NLTK's VADER sentiment analyzerto chech the positivity or negativity of the sentence.\n",
        "\n",
        "5. Named Entities:\n",
        "This feature classifies named entities such as products, dates, or organizations extracted from the text using SpaCy’s Named Entity Recognition (NER).\n",
        "\n",
        "6. Number of Specific Keywords:\n",
        "This feature counts how many times certain important keywords (such as “refund,” “password,” “support,” “upgrade,” “account”) appear in the ticket text.\n",
        "\n",
        "How These Features are helpful:\n",
        "\n",
        "-Text length: The length of a ticket gives insights into its complexity. Short tickets may indicate simple inquiries, while longer ones may involve more detailed complaints or technical issues.\n",
        "\n",
        "-Number of Exclamation Marks: Tickets with multiple exclamation marks could likely belong to complaints or urgent requests that require immediate attention.\n",
        "\n",
        "-Presence of Urgent Words: Urgent words signal that the customer expects a prompt response, and these tickets should be routed to high-priority teams.\n",
        "                          It can help in identifying tickets that require immediate attention.\n",
        "\n",
        "-Sentiment Score: A negative sentiment score might indicate dissatisfaction, while a positive score might represent a simple inquiry or positive feedback.\n",
        "                  This feature helps classify whether the customer is facing an issue or is simply requesting information.\n",
        "\n",
        "-Named Entities: Named entities help in understanding the specific product or service that the customer is referring to.\n",
        "\n",
        "-Number of Specific Keywords: Certain keywords are strong indicators of the ticket type. For instance, \"refund\" suggests a billing issue,\n",
        "                              while \"password\" indicates an account management problem.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca83d72-0e10-45ec-aa68-3c29eec0d093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 1 - Text Length (Word Count):\n",
            "                                                text  text_length\n",
            "0  Unable to login to my account I forgot my pass...           15\n",
            "1  Payment Issue! I was charged twice for my last...           15\n",
            "2  Product not working The vacuum cleaner I bough...           18\n",
            "3  Need upgrade assistance Can you help me upgrad...           15\n",
            "4  Complaint about service I am very disappointed...           16\n",
            "\n",
            "Feature 2 - Exclamation Marks Count:\n",
            "                                                text  exclamation_marks\n",
            "0  Unable to login to my account I forgot my pass...                  0\n",
            "1  Payment Issue! I was charged twice for my last...                  1\n",
            "2  Product not working The vacuum cleaner I bough...                  0\n",
            "3  Need upgrade assistance Can you help me upgrad...                  0\n",
            "4  Complaint about service I am very disappointed...                  0\n",
            "\n",
            "Feature 3 - Presence of Urgent Words:\n",
            "                                                text  urgent_flag\n",
            "0  Unable to login to my account I forgot my pass...        False\n",
            "1  Payment Issue! I was charged twice for my last...         True\n",
            "2  Product not working The vacuum cleaner I bough...        False\n",
            "3  Need upgrade assistance Can you help me upgrad...         True\n",
            "4  Complaint about service I am very disappointed...        False\n",
            "\n",
            "Feature 4 - Sentiment Scores:\n",
            "                                                text  sentiment\n",
            "0  Unable to login to my account I forgot my pass...     0.4019\n",
            "1  Payment Issue! I was charged twice for my last...     0.2003\n",
            "2  Product not working The vacuum cleaner I bough...    -0.0516\n",
            "3  Need upgrade assistance Can you help me upgrad...     0.4019\n",
            "4  Complaint about service I am very disappointed...    -0.6801\n",
            "\n",
            "Feature 5 - Named Entities:\n",
            "                                                text entities\n",
            "0  Unable to login to my account I forgot my pass...       []\n",
            "1  Payment Issue! I was charged twice for my last...       []\n",
            "2  Product not working The vacuum cleaner I bough...   [DATE]\n",
            "3  Need upgrade assistance Can you help me upgrad...       []\n",
            "4  Complaint about service I am very disappointed...       []\n",
            "\n",
            "Feature 6 - Specific Keywords:\n",
            "                                                text  keyword_count\n",
            "0  Unable to login to my account I forgot my pass...              2\n",
            "1  Payment Issue! I was charged twice for my last...              1\n",
            "2  Product not working The vacuum cleaner I bough...              0\n",
            "3  Need upgrade assistance Can you help me upgrad...              2\n",
            "4  Complaint about service I am very disappointed...              0\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "\n",
        "# Download NLTK VADER for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load Spacy's language model for Named Entity Recognition (NER)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample customer service ticket data\n",
        "data = [\n",
        "    {\"subject\": \"Unable to login to my account\", \"body\": \"I forgot my password and need help resetting it.\"},\n",
        "    {\"subject\": \"Payment Issue!\", \"body\": \"I was charged twice for my last purchase. Please issue a refund asap.\"},\n",
        "    {\"subject\": \"Product not working\", \"body\": \"The vacuum cleaner I bought is not turning on. It stopped working after a week.\"},\n",
        "    {\"subject\": \"Need upgrade assistance\", \"body\": \"Can you help me upgrade my software immediately to the latest version?\"},\n",
        "    {\"subject\": \"Complaint about service\", \"body\": \"I am very disappointed with the service. The response time is too slow.\"}\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Combine subject and body into a single text column for feature extraction\n",
        "df['text'] = df['subject'] + ' ' + df['body']\n",
        "\n",
        "# Feature 1: Length of the text (number of words)\n",
        "df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "print(\"Feature 1 - Text Length (Word Count):\")\n",
        "print(df[['text', 'text_length']])\n",
        "\n",
        "# Feature 2: Number of exclamation marks\n",
        "df['exclamation_marks'] = df['text'].apply(lambda x: x.count('!'))\n",
        "print(\"\\nFeature 2 - Exclamation Marks Count:\")\n",
        "print(df[['text', 'exclamation_marks']])\n",
        "\n",
        "# Feature 3: Presence of urgent words (like 'urgent', 'immediately', 'asap')\n",
        "urgent_words = ['urgent', 'immediately', 'asap']\n",
        "df['urgent_flag'] = df['text'].apply(lambda x: any(word in x.lower() for word in urgent_words))\n",
        "print(\"\\nFeature 3 - Presence of Urgent Words:\")\n",
        "print(df[['text', 'urgent_flag']])\n",
        "\n",
        "# Feature 4: Sentiment Analysis (using NLTK's VADER sentiment analysis)\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "df['sentiment'] = df['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "print(\"\\nFeature 4 - Sentiment Scores:\")\n",
        "print(df[['text', 'sentiment']])\n",
        "\n",
        "# Feature 5: Named Entities (using SpaCy NER for entities like product names or organizations)\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    return [ent.label_ for ent in doc.ents]  # Extract only entity labels\n",
        "\n",
        "df['entities'] = df['text'].apply(extract_entities)\n",
        "print(\"\\nFeature 5 - Named Entities:\")\n",
        "print(df[['text', 'entities']])\n",
        "\n",
        "# Feature 6: Number of specific keywords (like 'refund', 'password', 'support')\n",
        "keywords = ['refund', 'password', 'support', 'upgrade', 'account']\n",
        "df['keyword_count'] = df['text'].apply(lambda x: sum([x.lower().count(word) for word in keywords]))\n",
        "print(\"\\nFeature 6 - Specific Keywords:\")\n",
        "print(df[['text', 'keyword_count']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d019671-377d-4829-ab49-c4d014bb9aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ranked Features by Chi-Square Score:\n",
            "             Feature  Chi-Square Score   P-Value\n",
            "4      keyword_count          0.625000  0.429195\n",
            "2        urgent_flag          0.500000  0.479500\n",
            "0        text_length          0.333333  0.563703\n",
            "1  exclamation_marks          0.250000  0.617075\n",
            "3          sentiment          0.189825  0.663062\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "#importing required libraries and modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "# Assuming df is already created with features from the previous step\n",
        "\n",
        "# Step 1: Generate labels (1 for urgent, 0 for non-urgent)\n",
        "# We will assume tickets with urgent words or negative sentiment are \"urgent\"\n",
        "df['label'] = df.apply(lambda x: 1 if x['urgent_flag'] or x['sentiment'] < 0 else 0, axis=1)\n",
        "\n",
        "# Step 2: Select the features for Chi-Square testing\n",
        "# These are the features we want to test\n",
        "features = ['text_length', 'exclamation_marks', 'urgent_flag', 'sentiment', 'keyword_count']\n",
        "\n",
        "# Prepare the feature matrix (X) and target vector (y)\n",
        "X = df[features]\n",
        "y = df['label']\n",
        "\n",
        "# Chi2 expects non-negative values, so we scale the features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply Chi-Square feature selection\n",
        "chi_scores, p_values = chi2(X_scaled, y)\n",
        "\n",
        "# Step 4: Rank features based on Chi-Square score\n",
        "chi2_scores_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Chi-Square Score': chi_scores,\n",
        "    'P-Value': p_values\n",
        "}).sort_values(by='Chi-Square Score', ascending=False)\n",
        "\n",
        "print(\"\\nRanked Features by Chi-Square Score:\")\n",
        "print(chi2_scores_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6d3d9d-a3d3-4862-c263-abb17ee47621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents based on Similarity to the Query:\n",
            "                         subject  \\\n",
            "3        Need upgrade assistance   \n",
            "0  Unable to login to my account   \n",
            "1                 Payment Issue!   \n",
            "4        Complaint about service   \n",
            "2            Product not working   \n",
            "\n",
            "                                                body  similarity  \n",
            "3  Can you help me upgrade my software immediatel...    0.849565  \n",
            "0   I forgot my password and need help resetting it.    0.832687  \n",
            "1  I was charged twice for my last purchase. Plea...    0.813991  \n",
            "4  I am very disappointed with the service. The r...    0.765909  \n",
            "2  The vacuum cleaner I bought is not turning on....    0.758997  \n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample customer service ticket data (same as in previous code)\n",
        "data = [\n",
        "    {\"subject\": \"Unable to login to my account\", \"body\": \"I forgot my password and need help resetting it.\"},\n",
        "    {\"subject\": \"Payment Issue!\", \"body\": \"I was charged twice for my last purchase. Please issue a refund asap.\"},\n",
        "    {\"subject\": \"Product not working\", \"body\": \"The vacuum cleaner I bought is not turning on. It stopped working after a week.\"},\n",
        "    {\"subject\": \"Need upgrade assistance\", \"body\": \"Can you help me upgrade my software immediately to the latest version?\"},\n",
        "    {\"subject\": \"Complaint about service\", \"body\": \"I am very disappointed with the service. The response time is too slow.\"}\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Combine subject and body into a single text column for feature extraction\n",
        "df['text'] = df['subject'] + ' ' + df['body']\n",
        "\n",
        "# Define the query to match relevant documents\n",
        "query = \"Help me reset my password and account access\"\n",
        "\n",
        "# Function to encode text using BERT model\n",
        "def encode_text(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Encode query and text data\n",
        "query_embedding = encode_text(query)\n",
        "df['embedding'] = df['text'].apply(lambda x: encode_text(x))\n",
        "\n",
        "# Compute cosine similarity between the query and each document\n",
        "df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity(query_embedding, x.reshape(1, -1)).item())\n",
        "\n",
        "# Rank documents by similarity in descending order\n",
        "df = df.sort_values(by='similarity', ascending=False)\n",
        "\n",
        "# Display the ranked documents\n",
        "print(\"Ranked Documents based on Similarity to the Query:\")\n",
        "print(df[['subject', 'body', 'similarity']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Learning experience: Overall i learnt about data extraction from different websites through various methods and it was very exciting.\n",
        "Chalanges encountered: At first it was though to extract as so many websites were not giving access to the data. Then I learnt ways to overome it.\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}